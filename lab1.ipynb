{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "START_SYMBOL = '*'\n",
    "STOP_SYMBOL = 'STOP'\n",
    "RARE_SYMBOL = '_RARE_'\n",
    "RARE_WORD_MAX_FREQ = 5\n",
    "LOG_PROB_OF_ZERO = -1000\n",
    "\n",
    "DATA_PATH = 'data/'\n",
    "OUTPUT_PATH = 'output/'\n",
    "UD_DATA_PATH = 'ud_data/'\n",
    "UD_OUTPUT_PATH = 'ud_output/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Receives a list of tagged sentences and processes each sentence to generate a list of words and a list of tags.\n",
    "# Each sentence is a string of space separated \"WORD/TAG\" tokens, with a newline character in the end.\n",
    "# Remember to include start and stop symbols in yout returned lists, as defined by the constants START_SYMBOL and STOP_SYMBOL.\n",
    "# brown_words (the list of words) should be a list where every element is a list of the tags of a particular sentence.\n",
    "# brown_tags (the list of tags) should be a list where every element is a list of the tags of a particular sentence.\n",
    "def split_wordtags(brown_train):\n",
    "    brown_words = []\n",
    "    brown_tags = []\n",
    "    \n",
    "    words_count = []\n",
    "    tags_count = []\n",
    "    words_count.append(START_SYMBOL)\n",
    "    tags_count.append(START_SYMBOL)\n",
    "    for line in brown_train:\n",
    "        lineWords = line.split()\n",
    "        for wordWithTag in lineWords:\n",
    "            wordAndTag = wordWithTag.split(\"/\")\n",
    "            if len(wordAndTag) == 2:\n",
    "                words_count.append(wordAndTag[0])\n",
    "                tags_count.append(wordAndTag[1])\n",
    "            else:\n",
    "                str = wordAndTag[0]\n",
    "                for i in range(1, len(wordAndTag) - 1):\n",
    "                    str = str + \"/\"\n",
    "                    str = str + wordAndTag[i]\n",
    "                tags_count.append(wordAndTag[len(wordAndTag) - 1])\n",
    "                words_count.append(str)\n",
    "    tags_count.append(STOP_SYMBOL)\n",
    "    words_count.append(STOP_SYMBOL)\n",
    "    \n",
    "    brown_words.append(words_count)\n",
    "    brown_tags.append(tags_count)\n",
    "    return brown_words, brown_tags\n",
    "\n",
    "\n",
    "# This function takes tags from the training data and calculates tag trigram probabilities.\n",
    "# It returns a python dictionary where the keys are tuples that represent the tag trigram, and the values are the log probability of that trigram\n",
    "def calc_trigrams(brown_tags):\n",
    "    q_values = {}\n",
    "    \n",
    "    bigram_count = collections.defaultdict(int)\n",
    "    trigram_count = collections.defaultdict(int)\n",
    "\n",
    "    for sentence in brown_tags:\n",
    "        for bigram in nltk.bigrams(sentence):\n",
    "            bigram_count[bigram] += 1\n",
    "\n",
    "    for sentence in brown_tags:\n",
    "        for trigram in nltk.trigrams(sentence):\n",
    "            trigram_count[trigram] += 1\n",
    "    q_values = {k: math.log(float(v) / bigram_count[k[:2]], 2) for k, v in trigram_count.items()}\n",
    "    \n",
    "    return q_values\n",
    "\n",
    "# This function takes output from calc_trigrams() and outputs it in the proper format\n",
    "def q2_output(q_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    trigrams = q_values.keys()\n",
    "\n",
    "    # trigrams.sort()\n",
    "    trigrams = sorted(trigrams)\n",
    "    for trigram in trigrams:\n",
    "        output = \" \".join(['TRIGRAM', trigram[0], trigram[1], trigram[2], str(q_values[trigram])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "# Takes the words from the training data and returns a set of all of the words that occur more than 5 times (use RARE_WORD_MAX_FREQ)\n",
    "# brown_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# Note: words that appear exactly 5 times should be considered rare!\n",
    "def calc_known(brown_words):\n",
    "    known_words = set([])\n",
    "    words_c = collections.defaultdict(int)\n",
    "\n",
    "    for sentence in brown_words:\n",
    "        for word in sentence:\n",
    "            words_c[word] += 1\n",
    "\n",
    "    for word, c in words_c.items():\n",
    "        if c > RARE_WORD_MAX_FREQ:\n",
    "            known_words.add(word)\n",
    "    return known_words\n",
    "\n",
    "# Takes the words from the training data and a set of words that should not be replaced for '_RARE_'\n",
    "# Returns the equivalent to brown_words but replacing the unknown words by '_RARE_' (use RARE_SYMBOL constant)\n",
    "def replace_rare(brown_words, known_words):\n",
    "    brown_words_rare = brown_words\n",
    "    for i, sentence in enumerate(brown_words):\n",
    "        for j, word in enumerate(sentence):\n",
    "            if word not in known_words:\n",
    "                brown_words_rare[i][j] = RARE_SYMBOL\n",
    "    return brown_words_rare\n",
    "\n",
    "# This function takes the ouput from replace_rare and outputs it to a file\n",
    "def q3_output(rare, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in rare:\n",
    "        outfile.write(' '.join(sentence[2:-1]) + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "# Calculates emission probabilities and creates a set of all possible tags\n",
    "# The first return value is a python dictionary where each key is a tuple in which the first element is a word\n",
    "# and the second is a tag, and the value is the log probability of the emission of the word given the tag\n",
    "# The second return value is a set of all possible tags for this data set\n",
    "def calc_emission(brown_words_rare, brown_tags):\n",
    "    e_values = {}\n",
    "    taglist = set([])\n",
    "    \n",
    "    e_values_c = collections.defaultdict(int)\n",
    "    tags_c = collections.defaultdict(int)\n",
    "\n",
    "    for word_sentence, tag_sentence in zip(brown_words_rare, brown_tags):\n",
    "        for word, tag in zip(word_sentence, tag_sentence):\n",
    "            e_values_c[(word, tag)] += 1\n",
    "            tags_c[tag] += 1\n",
    "\n",
    "    for (word, tag), p in e_values_c.items():\n",
    "        e_values[(word, tag)] = math.log(float(p) / tags_c[tag], 2)\n",
    "    taglist = set(tags_c)\n",
    "    \n",
    "    return e_values, taglist\n",
    "\n",
    "# This function takes the output from calc_emissions() and outputs it\n",
    "def q4_output(e_values, filename):\n",
    "    outfile = open(filename, \"w\")\n",
    "    emissions = e_values.keys()\n",
    "    # emissions.sort()  for python 2\n",
    "    emissions = sorted(emissions)  \n",
    "    for item in emissions:\n",
    "        output = \" \".join([item[0], item[1], str(e_values[item])])\n",
    "        outfile.write(output + '\\n')\n",
    "    outfile.close()\n",
    "\n",
    "\n",
    "# This function takes data to tag (brown_dev_words), a set of all possible tags (taglist), a set of all known words (known_words),\n",
    "# trigram probabilities (q_values) and emission probabilities (e_values) and outputs a list where every element is a tagged sentence \n",
    "# (in the WORD/TAG format, separated by spaces and with a newline in the end, just like our input tagged data)\n",
    "# brown_dev_words is a python list where every element is a python list of the words of a particular sentence.\n",
    "# taglist is a set of all possible tags\n",
    "# known_words is a set of all known words\n",
    "# q_values is from the return of calc_trigrams()\n",
    "# e_values is from the return of calc_emissions()\n",
    "# The return value is a list of tagged sentences in the format \"WORD/TAG\", separated by spaces. Each sentence is a string with a \n",
    "# terminal newline, not a list of tokens. Remember also that the output should not contain the \"_RARE_\" symbol, but rather the\n",
    "# original words of the sentence!\n",
    "def viterbi(brown_dev_words, taglist, known_words, q_values, e_values):\n",
    "    tagged = []\n",
    "    pi = collections.defaultdict(float)\n",
    "    bp = {}\n",
    "    bp[(-1, START_SYMBOL, START_SYMBOL)] = START_SYMBOL\n",
    "    pi[(-1, START_SYMBOL, START_SYMBOL)] = 0.0\n",
    "\n",
    "    for tokens_orig in brown_dev_words:\n",
    "        tokens = [w if w in known_words else RARE_SYMBOL for w in tokens_orig]\n",
    "        for w in taglist:\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "            word_tag = (tokens[0], w)\n",
    "            trigram = (START_SYMBOL, START_SYMBOL, w)\n",
    "            pi[(0, START_SYMBOL, w)] = pi[(-1, START_SYMBOL, START_SYMBOL)] + q_values.get(trigram, LOG_PROB_OF_ZERO) + e_values.get(word_tag, LOG_PROB_OF_ZERO)\n",
    "            bp[(0, START_SYMBOL, w)] = START_SYMBOL\n",
    "\n",
    "        for w in taglist:\n",
    "            for u in taglist:\n",
    "                if len(tokens) < 2:\n",
    "                    continue\n",
    "                word_tag = (tokens[1], u)\n",
    "                trigram = (START_SYMBOL, w, u)\n",
    "                pi[(1, w, u)] = pi.get((0, START_SYMBOL, w), LOG_PROB_OF_ZERO) + q_values.get(trigram, LOG_PROB_OF_ZERO) + e_values.get(word_tag, LOG_PROB_OF_ZERO)\n",
    "                bp[(1, w, u)] = START_SYMBOL\n",
    "\n",
    "        for k in range(2, len(tokens)):\n",
    "            for u in taglist:\n",
    "                for v in taglist:\n",
    "                    max_prob = float('-Inf')\n",
    "                    max_tag = ''\n",
    "                    for w in taglist:\n",
    "                        score = pi.get((k - 1, w, u), LOG_PROB_OF_ZERO) + q_values.get((w, u, v), LOG_PROB_OF_ZERO) + e_values.get((tokens[k], v), LOG_PROB_OF_ZERO)\n",
    "                        if (score > max_prob):\n",
    "                            max_prob = score\n",
    "                            max_tag = w\n",
    "                    bp[(k, u, v)] = max_tag\n",
    "                    pi[(k, u, v)] = max_prob\n",
    "\n",
    "        max_prob = float('-Inf')\n",
    "        v_max, u_max = None, None\n",
    "        \n",
    "        for (u, v) in itertools.product(taglist, taglist):\n",
    "            score = pi.get((len(tokens_orig) - 1, u, v), LOG_PROB_OF_ZERO) + q_values.get((u, v, STOP_SYMBOL), LOG_PROB_OF_ZERO)\n",
    "            if score > max_prob:\n",
    "                max_prob = score\n",
    "                u_max = u\n",
    "                v_max = v\n",
    "                \n",
    "        tags = []\n",
    "        tags.append(v_max)\n",
    "        tags.append(u_max)\n",
    "\n",
    "        for count, k in enumerate(range(len(tokens_orig) - 3, -1, -1)):\n",
    "            tags.append(bp[(k + 2, tags[count + 1], tags[count])])\n",
    "\n",
    "        tagged_sentence = []\n",
    "        tags.reverse()\n",
    "\n",
    "        for k in range(0, len(tokens_orig)):\n",
    "            tagged_sentence += [tokens_orig[k], \"/\", str(tags[k]), \" \"]\n",
    "        tagged_sentence.append('\\n')\n",
    "        tagged.append(''.join(tagged_sentence))\n",
    "\n",
    "    return tagged\n",
    "\n",
    "\n",
    "# This function takes the output of viterbi() and outputs it to file\n",
    "def q5_output(tagged, filename):\n",
    "    outfile = open(filename, 'w')\n",
    "    for sentence in tagged:\n",
    "        outfile.write(sentence)\n",
    "    outfile.close()\n",
    "\n",
    "def pos(tagged, right_tegged):\n",
    "    infile = open(tagged, \"r\")\n",
    "    user_sentences = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    infile = open(right_tegged, \"r\")\n",
    "    correct_sentences = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    num_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for user_sent, correct_sent in zip(user_sentences, correct_sentences):\n",
    "        user_tok = user_sent.split()\n",
    "        correct_tok = correct_sent.split()\n",
    "\n",
    "        if len(user_tok) != len(correct_tok):\n",
    "            continue\n",
    "\n",
    "        for u, c in zip(user_tok, correct_tok):\n",
    "            if u == c:\n",
    "                num_correct += 1\n",
    "            total += 1\n",
    "\n",
    "    score = float(num_correct) / total * 100\n",
    "\n",
    "    print(\"Percent correct tags:\", score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def part_1():\n",
    "    # start timer\n",
    "    time.clock()\n",
    "\n",
    "    # open Brown training data\n",
    "    infile = open(DATA_PATH + \"Brown_tagged_train.txt\", \"r\")\n",
    "    brown_train = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    # split words and tags, and add start and stop symbols (question 1)\n",
    "    brown_words, brown_tags = split_wordtags(brown_train)\n",
    "\n",
    "    # calculate tag trigram probabilities (question 2)\n",
    "    q_values = calc_trigrams(brown_tags)\n",
    "\n",
    "    # question 2 output\n",
    "    q2_output(q_values, OUTPUT_PATH + 'B2.txt')\n",
    "\n",
    "    # calculate list of words with count > 5 (question 3)\n",
    "    known_words = calc_known(brown_words)\n",
    "\n",
    "    # get a version of brown_words with rare words replace with '_RARE_' (question 3)\n",
    "    brown_words_rare = replace_rare(brown_words, known_words)\n",
    "\n",
    "    # question 3 output\n",
    "    q3_output(brown_words_rare, OUTPUT_PATH + \"B3.txt\")\n",
    "\n",
    "    # calculate emission probabilities (question 4)\n",
    "    e_values, taglist = calc_emission(brown_words_rare, brown_tags)\n",
    "\n",
    "    # question 4 output\n",
    "    q4_output(e_values, OUTPUT_PATH + \"B4.txt\")\n",
    "\n",
    "    # delete unneceessary data\n",
    "    del brown_train\n",
    "    del brown_words_rare\n",
    "\n",
    "    # open Brown development data (question 5)\n",
    "    infile = open(DATA_PATH + \"Brown_dev.txt\", \"r\")\n",
    "    brown_dev = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    # format Brown development data here\n",
    "    brown_dev_words = []\n",
    "    for sentence in brown_dev:\n",
    "        brown_dev_words.append(sentence.split(\" \")[:-1])\n",
    "\n",
    "    # do viterbi on brown_dev_words (question 5)\n",
    "    viterbi_tagged = viterbi(brown_dev_words, taglist, known_words, q_values, e_values)\n",
    "\n",
    "    # question 5 output\n",
    "    q5_output(viterbi_tagged, OUTPUT_PATH + 'B5.txt')\n",
    "\n",
    "    # print total time to run Part B\n",
    "    print(\"Part B time: \",str(time.clock()),' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct tags: 90.79406648816621\n"
     ]
    }
   ],
   "source": [
    "pos(OUTPUT_PATH + \"B5.txt\", DATA_PATH + \"Brown_tagged_dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(input_filename, output_filename, with_tags=True):\n",
    "    file_r = open(input_filename, encoding='utf-8')\n",
    "    file_a = open(output_filename, 'a', encoding='utf-8')\n",
    "\n",
    "    temp = file_r.readline()\n",
    "    text_pattern = '# sentence-text: '\n",
    "    word_tag_pattern = '^\\\\d+\\\\s+([\\\\w\\\\.,’:-]+)\\\\s+[\\\\w\\\\.,’:-]+\\\\s+([\\\\w]+)'\n",
    "    while temp:\n",
    "        if len(re.findall(text_pattern, temp)) > 0:\n",
    "            output_string = ''\n",
    "            while True:\n",
    "                temp = file_r.readline()\n",
    "                result = re.findall(word_tag_pattern, temp)\n",
    "                if len(result) > 0:\n",
    "                    if with_tags:\n",
    "                        output_string += result[0][0] + '/' + result[0][1] + ' '\n",
    "                    else:\n",
    "                        output_string += result[0][0] + ' '\n",
    "                else:\n",
    "                    if temp == '\\n':\n",
    "                        output_string += '\\n'\n",
    "                        file_a.write(output_string)\n",
    "\n",
    "                        break\n",
    "\n",
    "        temp = file_r.readline()\n",
    "\n",
    "    file_r.close()\n",
    "    file_a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def part_2():\n",
    "    # start timer\n",
    "    time.clock()\n",
    "    \n",
    "    parsedData = parse(UD_DATA_PATH + \"en-ud-train.conllu\", UD_DATA_PATH + \"en-ud-train.txt\")\n",
    "    parsedData = parse(UD_DATA_PATH + \"en-ud-dev.conllu\", UD_DATA_PATH + \"en-ud-tagged-dev.txt\")\n",
    "    parsedData = parse(UD_DATA_PATH + \"en-ud-dev.conllu\", UD_DATA_PATH + \"en-ud-dev.txt\", False)\n",
    "    \n",
    "    # open Brown training data\n",
    "    infile = open(UD_DATA_PATH + \"en-ud-train.txt\", \"r\")\n",
    "    brown_train = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    # split words and tags, and add start and stop symbols (question 1)\n",
    "    brown_words, brown_tags = split_wordtags(brown_train)\n",
    "    \n",
    "    # calculate tag trigram probabilities (question 2)\n",
    "    q_values = calc_trigrams(brown_tags)\n",
    "\n",
    "    # question 2 output\n",
    "    q2_output(q_values, UD_OUTPUT_PATH + 'B2.txt')\n",
    "\n",
    "    # calculate list of words with count > 5 (question 3)\n",
    "    known_words = calc_known(brown_words)\n",
    "\n",
    "    # get a version of brown_words with rare words replace with '_RARE_' (question 3)\n",
    "    brown_words_rare = replace_rare(brown_words, known_words)\n",
    "\n",
    "    # question 3 output\n",
    "    q3_output(brown_words_rare, UD_OUTPUT_PATH + \"B3.txt\")\n",
    "\n",
    "    # calculate emission probabilities (question 4)\n",
    "    e_values, taglist = calc_emission(brown_words_rare, brown_tags)\n",
    "\n",
    "    # question 4 output\n",
    "    q4_output(e_values, UD_OUTPUT_PATH + \"B4.txt\")\n",
    "\n",
    "    # delete unneceessary data\n",
    "    del brown_train\n",
    "    del brown_words_rare\n",
    "\n",
    "    # open Brown development data (question 5)\n",
    "    infile = open(UD_DATA_PATH + \"en-ud-dev.txt\", \"r\")\n",
    "    brown_dev = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    # format Brown development data here\n",
    "    brown_dev_words = []\n",
    "    for sentence in brown_dev:\n",
    "        brown_dev_words.append(sentence.split(\" \")[:-1])\n",
    "\n",
    "    # do viterbi on brown_dev_words (question 5)\n",
    "    viterbi_tagged = viterbi(brown_dev_words, taglist, known_words, q_values, e_values)\n",
    "\n",
    "    # question 5 output\n",
    "    q5_output(viterbi_tagged, UD_OUTPUT_PATH + 'B5.txt')\n",
    "\n",
    "    # print total time to run Part B\n",
    "    print(\"Part B time: \",str(time.clock()),' sec')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct tags: 85.51441018766755\n"
     ]
    }
   ],
   "source": [
    "pos(UD_OUTPUT_PATH + \"B5.txt\", UD_DATA_PATH + \"en-ud-tagged-dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crf(brown_words_rare, brown_tags, brown_dev_words):\n",
    "    tagged = []\n",
    "    ct = nltk.CRFTagger()\n",
    "    train_data = []\n",
    "    for i in range(len(brown_words_rare)):\n",
    "        train_data.append([])\n",
    "        for j in range(len(brown_words_rare[i])):\n",
    "            train_data[i].append((brown_words_rare[i][j], brown_tags[i][j]))\n",
    "    ct.train(train_data, 'temp.dat')\n",
    "\n",
    "    tagged_sentences = ct.tag_sents(brown_dev_words)\n",
    "    for sentence in tagged_sentences:\n",
    "        tagged_sentence = []\n",
    "        for word_tag_pair in sentence:\n",
    "            tagged_sentence += [word_tag_pair[0], \"/\", word_tag_pair[1], \" \"]\n",
    "        tagged_sentence.append('\\n')\n",
    "        tagged.append(''.join(tagged_sentence))\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def part_3():\n",
    "    # start timer\n",
    "    time.clock()\n",
    "    \n",
    "    parsedData = parse(UD_DATA_PATH + \"en-ud-train.conllu\", UD_DATA_PATH + \"en-ud-train.txt\")\n",
    "    parsedData = parse(UD_DATA_PATH + \"en-ud-dev.conllu\", UD_DATA_PATH + \"en-ud-tagged-dev.txt\")\n",
    "    parsedData = parse(UD_DATA_PATH + \"en-ud-dev.conllu\", UD_DATA_PATH + \"en-ud-dev.txt\", False)\n",
    "    \n",
    "    # open Brown training data\n",
    "    infile = open(UD_DATA_PATH + \"en-ud-train.txt\", \"r\")\n",
    "    brown_train = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    brown_words, brown_tags = split_wordtags(brown_train)\n",
    "\n",
    "    # calculate tag trigram probabilities (question 2)\n",
    "    print('Calculating trigrams')\n",
    "    q_values = calc_trigrams(brown_tags)\n",
    "\n",
    "    # question 2 output\n",
    "    q2_output(q_values, UD_OUTPUT_PATH + '/B2_crf.txt')\n",
    "\n",
    "    # calculate list of words with count > 5 (question 3)\n",
    "    known_words = calc_known(brown_words)\n",
    "\n",
    "    # get a version of brown_words with rare words replace with '_RARE_' (question 3)\n",
    "    brown_words_rare = replace_rare(brown_words, known_words)\n",
    "\n",
    "    # question 3 output\n",
    "    q3_output(brown_words_rare, UD_OUTPUT_PATH + \"/B3_crf.txt\")\n",
    "\n",
    "    # calculate emission probabilities (question 4)\n",
    "    e_values, taglist = calc_emission(brown_words_rare, brown_tags)\n",
    "\n",
    "    # question 4 output\n",
    "    q4_output(e_values, UD_OUTPUT_PATH  + \"/B4_crf.txt\")\n",
    "\n",
    "    # delete unneceessary data\n",
    "    del brown_train\n",
    "\n",
    "    # open Brown development data (question 5)\n",
    "    infile = open(UD_DATA_PATH + \"en-ud-dev.txt\", \"r\")\n",
    "    brown_dev = infile.readlines()\n",
    "    infile.close()\n",
    "\n",
    "    # format Brown development data here\n",
    "    brown_dev_words = []\n",
    "    for sentence in brown_dev:\n",
    "        brown_dev_words.append(sentence.split(\" \")[:-1])\n",
    "\n",
    "    crf_tagged = crf(brown_words_rare, brown_tags, brown_dev_words)\n",
    "    q5_output(crf_tagged, UD_OUTPUT_PATH + '/B5_crf.txt')\n",
    "\n",
    "    # print total time to run Part B\n",
    "    print(\"Part B time: \", str(time.clock()), ' sec')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating trigrams\n",
      "Calculating known words\n",
      "Replacing rare words\n",
      "Calculating emission probabilities\n",
      "CRF processing\n",
      "Part B time:  3457.65144370234  sec\n"
     ]
    }
   ],
   "source": [
    "part_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent correct tags: 92.0995308310992\n"
     ]
    }
   ],
   "source": [
    "pos(UD_OUTPUT_PATH + \"B5_crf.txt\", UD_DATA_PATH + \"en-ud-tagged-dev.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
